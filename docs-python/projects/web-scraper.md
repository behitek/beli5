# ğŸ•·ï¸ Web Scraper - Thu Tháº­p Dá»¯ Liá»‡u Tá»« Web

:::info
XÃ¢y dá»±ng má»™t bot thu tháº­p dá»¯ liá»‡u thÃ´ng minh tá»« cÃ¡c trang web - giá»‘ng nhÆ° má»™t thÃ¡m tá»­ ká»¹ thuáº­t sá»‘!
:::

## ğŸ¯ Má»¥c tiÃªu dá»± Ã¡n

Táº¡o má»™t Web Scraper cÃ³ thá»ƒ:
- ğŸŒ **Thu tháº­p dá»¯ liá»‡u** tá»« cÃ¡c trang web
- ğŸ“Š **PhÃ¢n tÃ­ch vÃ  xá»­ lÃ½** dá»¯ liá»‡u thu tháº­p Ä‘Æ°á»£c
- ğŸ’¾ **LÆ°u trá»¯** vÃ o nhiá»u Ä‘á»‹nh dáº¡ng (JSON, CSV, Database)
- ğŸ”„ **Láº­p lá»‹ch** thu tháº­p tá»± Ä‘á»™ng
- ğŸ›¡ï¸ **Xá»­ lÃ½ lá»—i** vÃ  retry thÃ´ng minh
- ğŸ“ˆ **Thá»‘ng kÃª** vÃ  bÃ¡o cÃ¡o chi tiáº¿t

## ğŸ› ï¸ Chuáº©n bá»‹

### ThÆ° viá»‡n cáº§n thiáº¿t

```python
import requests
import time
import json
import csv
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import pandas as pd
from dataclasses import dataclass
from typing import List, Dict, Optional
import logging
import sqlite3
from concurrent.futures import ThreadPoolExecutor
import schedule
import random
from fake_useragent import UserAgent
```

### CÃ i Ä‘áº·t thÆ° viá»‡n

```bash
pip install requests beautifulsoup4 pandas fake-useragent schedule lxml
```

## ğŸ—ï¸ XÃ¢y dá»±ng Web Scraper

### 1. Base Scraper Class

```python
import requests
from bs4 import BeautifulSoup
import time
import json
from datetime import datetime
import logging
from fake_useragent import UserAgent
from urllib.parse import urljoin, urlparse
import random

class WebScraper:
    """
    Base Web Scraper class
    Giá»‘ng nhÆ° má»™t thÃ¡m tá»­ ká»¹ thuáº­t sá»‘ thÃ´ng minh!
    """
    
    def __init__(self, base_url="", delay_range=(1, 3), timeout=10):
        self.base_url = base_url
        self.delay_range = delay_range
        self.timeout = timeout
        self.session = requests.Session()
        self.ua = UserAgent()
        
        # Statistics
        self.requests_made = 0
        self.successful_requests = 0
        self.failed_requests = 0
        self.start_time = datetime.now()
        
        # Setup logging
        self.setup_logging()
        
        # Setup session headers
        self.setup_session()
    
    def setup_logging(self):
        """Thiáº¿t láº­p logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('scraper.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def setup_session(self):
        """Thiáº¿t láº­p session vá»›i headers giáº£ láº­p"""
        self.session.headers.update({
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
    
    def random_delay(self):
        """Delay ngáº«u nhiÃªn Ä‘á»ƒ trÃ¡nh bá»‹ block"""
        delay = random.uniform(self.delay_range[0], self.delay_range[1])
        time.sleep(delay)
    
    def make_request(self, url, max_retries=3):
        """Thá»±c hiá»‡n request vá»›i retry logic"""
        self.requests_made += 1
        
        for attempt in range(max_retries):
            try:
                self.logger.info(f"ğŸŒ Requesting: {url} (Attempt {attempt + 1})")
                
                # Random User-Agent cho má»—i request
                self.session.headers['User-Agent'] = self.ua.random
                
                response = self.session.get(url, timeout=self.timeout)
                response.raise_for_status()
                
                self.successful_requests += 1
                self.logger.info(f"âœ… Success: {url}")
                
                # Random delay
                self.random_delay()
                
                return response
                
            except requests.exceptions.RequestException as e:
                self.logger.warning(f"âš ï¸ Attempt {attempt + 1} failed for {url}: {e}")
                
                if attempt < max_retries - 1:
                    # Exponential backoff
                    wait_time = (2 ** attempt) + random.uniform(1, 3)
                    time.sleep(wait_time)
                else:
                    self.failed_requests += 1
                    self.logger.error(f"âŒ All attempts failed for {url}")
                    return None
    
    def parse_html(self, html_content):
        """Parse HTML content"""
        return BeautifulSoup(html_content, 'html.parser')
    
    def get_absolute_url(self, relative_url):
        """Chuyá»ƒn relative URL thÃ nh absolute URL"""
        return urljoin(self.base_url, relative_url)
    
    def extract_text(self, element):
        """Extract vÃ  clean text tá»« element"""
        if element:
            return element.get_text(strip=True)
        return ""
    
    def extract_links(self, soup, selector="a[href]"):
        """Extract táº¥t cáº£ links tá»« page"""
        links = []
        for link in soup.select(selector):
            href = link.get('href')
            if href:
                absolute_url = self.get_absolute_url(href)
                links.append({
                    'url': absolute_url,
                    'text': self.extract_text(link),
                    'title': link.get('title', '')
                })
        return links
    
    def get_statistics(self):
        """Láº¥y thá»‘ng kÃª scraping"""
        runtime = datetime.now() - self.start_time
        
        return {
            'requests_made': self.requests_made,
            'successful_requests': self.successful_requests,
            'failed_requests': self.failed_requests,
            'success_rate': (self.successful_requests / self.requests_made * 100) if self.requests_made > 0 else 0,
            'runtime': str(runtime),
            'requests_per_minute': self.requests_made / (runtime.total_seconds() / 60) if runtime.total_seconds() > 0 else 0
        }
    
    def save_to_json(self, data, filename):
        """LÆ°u dá»¯ liá»‡u vÃ o JSON file"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2, default=str)
        self.logger.info(f"ğŸ’¾ Saved data to {filename}")
    
    def save_to_csv(self, data, filename):
        """LÆ°u dá»¯ liá»‡u vÃ o CSV file"""
        if data:
            df = pd.DataFrame(data)
            df.to_csv(filename, index=False, encoding='utf-8')
            self.logger.info(f"ğŸ“Š Saved data to {filename}")

print("âœ… WebScraper base class Ä‘Ã£ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a!")
```

### 2. News Scraper - Thu tháº­p tin tá»©c

```python
from dataclasses import dataclass
from typing import List
import re

@dataclass
class NewsArticle:
    """Data class cho bÃ i bÃ¡o"""
    title: str
    url: str
    summary: str
    author: str
    publish_date: str
    category: str
    content: str = ""
    tags: List[str] = None
    
    def __post_init__(self):
        if self.tags is None:
            self.tags = []

class NewsScraper(WebScraper):
    """
    News Scraper chuyÃªn thu tháº­p tin tá»©c
    Giá»‘ng nhÆ° má»™t phÃ³ng viÃªn robot!
    """
    
    def __init__(self, base_url):
        super().__init__(base_url)
        self.articles = []
    
    def scrape_vnexpress_homepage(self):
        """Scrape trang chá»§ VnExpress (demo)"""
        self.logger.info("ğŸ“° Báº¯t Ä‘áº§u scrape VnExpress homepage...")
        
        response = self.make_request(self.base_url)
        if not response:
            return []
        
        soup = self.parse_html(response.content)
        articles = []
        
        # Scrape main articles
        main_articles = soup.select('.item-news')
        
        for article in main_articles[:10]:  # Láº¥y 10 bÃ i Ä‘áº§u
            try:
                # Extract title and URL
                title_element = article.select_one('.title-news a')
                if not title_element:
                    continue
                
                title = self.extract_text(title_element)
                url = title_element.get('href', '')
                
                # Extract summary
                summary_element = article.select_one('.description a')
                summary = self.extract_text(summary_element) if summary_element else ""
                
                # Extract category tá»« URL hoáº·c class
                category = self.extract_category_from_url(url)
                
                # Extract publish date
                date_element = article.select_one('.time')
                publish_date = self.extract_text(date_element) if date_element else ""
                
                article_data = NewsArticle(
                    title=title,
                    url=url,
                    summary=summary,
                    author="VnExpress",
                    publish_date=publish_date,
                    category=category
                )
                
                articles.append(article_data)
                self.logger.info(f"ğŸ“„ Scraped article: {title[:50]}...")
                
            except Exception as e:
                self.logger.error(f"âŒ Error scraping article: {e}")
                continue
        
        self.articles.extend(articles)
        self.logger.info(f"âœ… Scraped {len(articles)} articles from homepage")
        return articles
    
    def extract_category_from_url(self, url):
        """Extract category tá»« URL"""
        if '/the-thao/' in url:
            return "Thá»ƒ thao"
        elif '/giai-tri/' in url:
            return "Giáº£i trÃ­"
        elif '/kinh-doanh/' in url:
            return "Kinh doanh"
        elif '/khoa-hoc/' in url:
            return "Khoa há»c"
        elif '/suc-khoe/' in url:
            return "Sá»©c khá»e"
        elif '/du-lich/' in url:
            return "Du lá»‹ch"
        else:
            return "Tin tá»©c"
    
    def scrape_article_content(self, article_url):
        """Scrape ná»™i dung chi tiáº¿t cá»§a bÃ i bÃ¡o"""
        response = self.make_request(article_url)
        if not response:
            return ""
        
        soup = self.parse_html(response.content)
        
        # Extract content paragraphs
        content_elements = soup.select('.fck_detail p')
        content_parts = []
        
        for p in content_elements:
            text = self.extract_text(p)
            if text and len(text) > 20:  # Ignore short paragraphs
                content_parts.append(text)
        
        return "\n\n".join(content_parts)
    
    def scrape_detailed_articles(self, max_articles=5):
        """Scrape ná»™i dung chi tiáº¿t cho cÃ¡c bÃ i bÃ¡o"""
        self.logger.info(f"ğŸ“š Scraping detailed content for {max_articles} articles...")
        
        for i, article in enumerate(self.articles[:max_articles]):
            self.logger.info(f"ğŸ“– Scraping content for: {article.title[:50]}...")
            
            content = self.scrape_article_content(article.url)
            article.content = content
            
            # Extract tags tá»« content
            article.tags = self.extract_tags_from_content(content)
            
            if (i + 1) % 3 == 0:  # Progress update
                self.logger.info(f"â³ Progress: {i + 1}/{max_articles} articles processed")
    
    def extract_tags_from_content(self, content):
        """Extract tags tá»« ná»™i dung bÃ i bÃ¡o"""
        if not content:
            return []
        
        # Simple keyword extraction
        keywords = {
            'cÃ´ng nghá»‡': ['cÃ´ng nghá»‡', 'technology', 'AI', 'robot', 'smartphone'],
            'kinh táº¿': ['kinh táº¿', 'economy', 'GDP', 'Ä‘áº§u tÆ°', 'chá»©ng khoÃ¡n'],
            'thá»ƒ thao': ['bÃ³ng Ä‘Ã¡', 'tennis', 'Olympic', 'World Cup', 'thá»ƒ thao'],
            'giáº£i trÃ­': ['phim', 'nháº¡c', 'ca sÄ©', 'diá»…n viÃªn', 'show'],
            'chÃ­nh trá»‹': ['chÃ­nh phá»§', 'quá»‘c há»™i', 'tá»•ng thá»‘ng', 'bá»™ trÆ°á»Ÿng']
        }
        
        tags = []
        content_lower = content.lower()
        
        for category, words in keywords.items():
            for word in words:
                if word.lower() in content_lower:
                    if category not in tags:
                        tags.append(category)
                    break
        
        return tags
    
    def get_articles_by_category(self, category):
        """Láº¥y articles theo category"""
        return [article for article in self.articles if article.category == category]
    
    def get_articles_by_date(self, date_str):
        """Láº¥y articles theo ngÃ y"""
        return [article for article in self.articles if date_str in article.publish_date]
    
    def search_articles(self, keyword):
        """TÃ¬m kiáº¿m articles theo keyword"""
        keyword_lower = keyword.lower()
        results = []
        
        for article in self.articles:
            if (keyword_lower in article.title.lower() or
                keyword_lower in article.summary.lower() or
                keyword_lower in article.content.lower()):
                results.append(article)
        
        return results
    
    def generate_report(self):
        """Táº¡o bÃ¡o cÃ¡o scraping"""
        if not self.articles:
            return "KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ táº¡o bÃ¡o cÃ¡o"
        
        # Category distribution
        categories = {}
        for article in self.articles:
            categories[article.category] = categories.get(article.category, 0) + 1
        
        # Tags distribution
        all_tags = []
        for article in self.articles:
            all_tags.extend(article.tags)
        
        tag_count = {}
        for tag in all_tags:
            tag_count[tag] = tag_count.get(tag, 0) + 1
        
        report = {
            'summary': {
                'total_articles': len(self.articles),
                'scraping_time': datetime.now().isoformat(),
                'categories': len(categories),
                'unique_tags': len(tag_count)
            },
            'category_distribution': categories,
            'top_tags': sorted(tag_count.items(), key=lambda x: x[1], reverse=True)[:10],
            'scraping_stats': self.get_statistics()
        }
        
        return report
    
    def export_data(self, format='json', filename=None):
        """Export dá»¯ liá»‡u ra file"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"news_data_{timestamp}"
        
        if format.lower() == 'json':
            # Convert dataclass to dict for JSON serialization
            articles_dict = []
            for article in self.articles:
                articles_dict.append({
                    'title': article.title,
                    'url': article.url,
                    'summary': article.summary,
                    'author': article.author,
                    'publish_date': article.publish_date,
                    'category': article.category,
                    'content': article.content[:500] + "..." if len(article.content) > 500 else article.content,
                    'tags': article.tags
                })
            
            self.save_to_json(articles_dict, f"{filename}.json")
            
        elif format.lower() == 'csv':
            # Convert to simple dict for CSV
            articles_dict = []
            for article in self.articles:
                articles_dict.append({
                    'title': article.title,
                    'url': article.url,
                    'summary': article.summary[:200] + "..." if len(article.summary) > 200 else article.summary,
                    'author': article.author,
                    'publish_date': article.publish_date,
                    'category': article.category,
                    'tags': ", ".join(article.tags)
                })
            
            self.save_to_csv(articles_dict, f"{filename}.csv")

# Demo News Scraper
def demo_news_scraper():
    """Demo News Scraper"""
    print("ğŸ“° DEMO NEWS SCRAPER")
    print("=" * 50)
    
    # Táº¡o scraper (demo vá»›i URL giáº£ láº­p)
    scraper = NewsScraper("https://vnexpress.net")
    
    # Giáº£ láº­p viá»‡c scrape (vÃ¬ khÃ´ng thá»ƒ thá»±c sá»± scrape VnExpress)
    print("ğŸ”„ Äang scrape homepage...")
    
    # Táº¡o dá»¯ liá»‡u demo
    demo_articles = [
        NewsArticle(
            title="CÃ´ng nghá»‡ AI phÃ¡t triá»ƒn máº¡nh máº½ trong nÄƒm 2024",
            url="https://vnexpress.net/cong-nghe-ai-2024",
            summary="TrÃ­ tuá»‡ nhÃ¢n táº¡o tiáº¿p tá»¥c cÃ³ nhá»¯ng bÆ°á»›c tiáº¿n vÆ°á»£t báº­c...",
            author="VnExpress",
            publish_date="15/03/2024",
            category="CÃ´ng nghá»‡",
            content="Trong nÄƒm 2024, cÃ´ng nghá»‡ AI Ä‘Ã£ cÃ³ nhá»¯ng bÆ°á»›c tiáº¿n Ä‘Ã¡ng ká»ƒ. CÃ¡c á»©ng dá»¥ng AI Ä‘Æ°á»£c tÃ­ch há»£p vÃ o nhiá»u lÄ©nh vá»±c khÃ¡c nhau tá»« y táº¿, giÃ¡o dá»¥c Ä‘áº¿n kinh doanh. Äáº·c biá»‡t, cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n nhÆ° GPT-4 Ä‘Ã£ cho tháº¥y kháº£ nÄƒng xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn áº¥n tÆ°á»£ng.",
            tags=["cÃ´ng nghá»‡", "AI", "2024"]
        ),
        NewsArticle(
            title="Thá»‹ trÆ°á»ng chá»©ng khoÃ¡n Viá»‡t Nam tÄƒng trÆ°á»Ÿng tÃ­ch cá»±c",
            url="https://vnexpress.net/chung-khoan-vn-tang-truong",
            summary="VN-Index Ä‘áº¡t má»©c tÄƒng trÆ°á»Ÿng áº¥n tÆ°á»£ng trong quÃ½ Ä‘áº§u nÄƒm...",
            author="VnExpress",
            publish_date="14/03/2024",
            category="Kinh doanh",
            content="Thá»‹ trÆ°á»ng chá»©ng khoÃ¡n Viá»‡t Nam ghi nháº­n sá»± tÄƒng trÆ°á»Ÿng máº¡nh máº½ vá»›i VN-Index vÆ°á»£t má»‘c 1,200 Ä‘iá»ƒm. CÃ¡c nhÃ  Ä‘áº§u tÆ° trong nÆ°á»›c vÃ  ngoáº¡i quá»‘c Ä‘á»u tá» ra láº¡c quan vá» triá»ƒn vá»ng kinh táº¿.",
            tags=["kinh táº¿", "chá»©ng khoÃ¡n", "Ä‘áº§u tÆ°"]
        ),
        NewsArticle(
            title="Äá»™i tuyá»ƒn bÃ³ng Ä‘Ã¡ Viá»‡t Nam chuáº©n bá»‹ cho World Cup 2026",
            url="https://vnexpress.net/dt-vn-world-cup-2026",
            summary="HLV Park Hang-seo Ä‘ang xÃ¢y dá»±ng káº¿ hoáº¡ch dÃ i háº¡n...",
            author="VnExpress",
            publish_date="13/03/2024",
            category="Thá»ƒ thao",
            content="Äá»™i tuyá»ƒn quá»‘c gia Viá»‡t Nam Ä‘ang tÃ­ch cá»±c chuáº©n bá»‹ cho vÃ²ng loáº¡i World Cup 2026. Vá»›i sá»± dáº«n dáº¯t cá»§a HLV Park Hang-seo, Ä‘á»™i tuyá»ƒn Ä‘ang Ä‘Æ°á»£c Ä‘áº§u tÆ° máº¡nh máº½ vá» nhÃ¢n sá»± vÃ  chiáº¿n thuáº­t.",
            tags=["thá»ƒ thao", "bÃ³ng Ä‘Ã¡", "World Cup"]
        )
    ]
    
    scraper.articles = demo_articles
    
    print(f"âœ… Scraped {len(demo_articles)} articles")
    
    # Demo cÃ¡c tÃ­nh nÄƒng
    print(f"\nğŸ” TÃNH NÄ‚NG SEARCH:")
    ai_articles = scraper.search_articles("AI")
    print(f"Found {len(ai_articles)} articles about AI:")
    for article in ai_articles:
        print(f"  ğŸ“„ {article.title}")
    
    print(f"\nğŸ“Š BÃO CÃO:")
    report = scraper.generate_report()
    print(f"Tá»•ng sá»‘ bÃ i: {report['summary']['total_articles']}")
    print(f"PhÃ¢n bá»‘ danh má»¥c:")
    for category, count in report['category_distribution'].items():
        print(f"  {category}: {count} bÃ i")
    
    # Export data
    print(f"\nğŸ’¾ EXPORT DATA:")
    scraper.export_data('json', 'demo_news')
    scraper.export_data('csv', 'demo_news')
    
    print(f"\nğŸ“ˆ THá»NG KÃŠ SCRAPING:")
    stats = scraper.get_statistics()
    for key, value in stats.items():
        print(f"  {key}: {value}")

# Cháº¡y demo
demo_news_scraper()
```

### 3. E-commerce Scraper - Thu tháº­p sáº£n pháº©m

```python
@dataclass
class Product:
    """Data class cho sáº£n pháº©m"""
    name: str
    price: float
    original_price: float
    discount_percent: float
    rating: float
    review_count: int
    url: str
    image_url: str
    description: str
    seller: str
    category: str
    in_stock: bool
    specifications: Dict[str, str] = None
    
    def __post_init__(self):
        if self.specifications is None:
            self.specifications = {}

class EcommerceScraper(WebScraper):
    """
    E-commerce Scraper cho cÃ¡c trang thÆ°Æ¡ng máº¡i Ä‘iá»‡n tá»­
    Giá»‘ng nhÆ° má»™t shopping assistant thÃ´ng minh!
    """
    
    def __init__(self, base_url, site_name="Generic"):
        super().__init__(base_url)
        self.site_name = site_name
        self.products = []
    
    def scrape_product_list(self, search_query, max_pages=3):
        """Scrape danh sÃ¡ch sáº£n pháº©m tá»« káº¿t quáº£ tÃ¬m kiáº¿m"""
        self.logger.info(f"ğŸ›’ Scraping products for query: {search_query}")
        
        for page in range(1, max_pages + 1):
            self.logger.info(f"ğŸ“„ Scraping page {page}...")
            
            # Construct search URL (tÃ¹y thuá»™c vÃ o site)
            search_url = self.construct_search_url(search_query, page)
            
            response = self.make_request(search_url)
            if not response:
                continue
            
            soup = self.parse_html(response.content)
            
            # Extract products tá»« page
            products = self.extract_products_from_page(soup)
            self.products.extend(products)
            
            self.logger.info(f"âœ… Scraped {len(products)} products from page {page}")
            
            # Check if there are more pages
            if not self.has_next_page(soup):
                break
    
    def construct_search_url(self, query, page):
        """Táº¡o URL tÃ¬m kiáº¿m (cáº§n customize cho tá»«ng site)"""
        # Demo URL format
        return f"{self.base_url}/search?q={query}&page={page}"
    
    def extract_products_from_page(self, soup):
        """Extract products tá»« má»™t page (demo implementation)"""
        products = []
        
        # Demo selectors (cáº§n thay Ä‘á»•i theo site thá»±c táº¿)
        product_elements = soup.select('.product-item')
        
        for element in product_elements:
            try:
                product = self.extract_single_product(element)
                if product:
                    products.append(product)
            except Exception as e:
                self.logger.error(f"âŒ Error extracting product: {e}")
                continue
        
        return products
    
    def extract_single_product(self, element):
        """Extract thÃ´ng tin tá»« má»™t product element"""
        # Demo extraction (cáº§n customize cho tá»«ng site)
        try:
            name = self.extract_text(element.select_one('.product-name'))
            if not name:
                return None
            
            # Price extraction
            price_text = self.extract_text(element.select_one('.price'))
            price = self.parse_price(price_text)
            
            # Original price (náº¿u cÃ³ discount)
            original_price_text = self.extract_text(element.select_one('.original-price'))
            original_price = self.parse_price(original_price_text) if original_price_text else price
            
            # Calculate discount
            discount_percent = ((original_price - price) / original_price * 100) if original_price > price else 0
            
            # Rating
            rating_element = element.select_one('.rating')
            rating = self.parse_rating(rating_element) if rating_element else 0
            
            # Review count
            review_text = self.extract_text(element.select_one('.review-count'))
            review_count = self.parse_review_count(review_text)
            
            # URL
            url_element = element.select_one('a[href]')
            url = self.get_absolute_url(url_element.get('href')) if url_element else ""
            
            # Image
            img_element = element.select_one('img')
            image_url = img_element.get('src') if img_element else ""
            
            product = Product(
                name=name,
                price=price,
                original_price=original_price,
                discount_percent=round(discount_percent, 2),
                rating=rating,
                review_count=review_count,
                url=url,
                image_url=image_url,
                description="",  # Sáº½ scrape sau náº¿u cáº§n
                seller=self.site_name,
                category="",
                in_stock=True  # Assume in stock if displayed
            )
            
            return product
            
        except Exception as e:
            self.logger.error(f"âŒ Error extracting single product: {e}")
            return None
    
    def parse_price(self, price_text):
        """Parse price tá»« text"""
        if not price_text:
            return 0.0
        
        # Remove currency symbols and format
        import re
        numbers = re.findall(r'[\d,]+', price_text.replace('.', ','))
        if numbers:
            return float(numbers[0].replace(',', ''))
        return 0.0
    
    def parse_rating(self, rating_element):
        """Parse rating tá»« element"""
        # Try to extract from stars, data attributes, etc.
        rating_text = self.extract_text(rating_element)
        
        import re
        rating_match = re.search(r'(\d+\.?\d*)', rating_text)
        if rating_match:
            return float(rating_match.group(1))
        
        # Try star counting
        stars = rating_element.select('.star.filled, .star.active')
        if stars:
            return len(stars)
        
        return 0.0
    
    def parse_review_count(self, review_text):
        """Parse review count tá»« text"""
        if not review_text:
            return 0
        
        import re
        numbers = re.findall(r'\d+', review_text.replace(',', ''))
        if numbers:
            return int(numbers[0])
        return 0
    
    def has_next_page(self, soup):
        """Kiá»ƒm tra cÃ²n trang tiáº¿p theo khÃ´ng"""
        # Look for pagination indicators
        next_button = soup.select_one('.next-page, .pagination .next')
        return next_button is not None and not next_button.get('disabled')
    
    def scrape_product_details(self, max_products=5):
        """Scrape chi tiáº¿t sáº£n pháº©m"""
        self.logger.info(f"ğŸ” Scraping details for {max_products} products...")
        
        for i, product in enumerate(self.products[:max_products]):
            if not product.url:
                continue
            
            self.logger.info(f"ğŸ“¦ Scraping details for: {product.name[:50]}...")
            
            response = self.make_request(product.url)
            if not response:
                continue
            
            soup = self.parse_html(response.content)
            
            # Extract detailed info
            product.description = self.extract_product_description(soup)
            product.specifications = self.extract_specifications(soup)
            product.category = self.extract_category(soup)
            
            if (i + 1) % 3 == 0:
                self.logger.info(f"â³ Progress: {i + 1}/{max_products} products detailed")
    
    def extract_product_description(self, soup):
        """Extract mÃ´ táº£ sáº£n pháº©m"""
        desc_element = soup.select_one('.product-description, .description, .detail')
        return self.extract_text(desc_element) if desc_element else ""
    
    def extract_specifications(self, soup):
        """Extract thÃ´ng sá»‘ ká»¹ thuáº­t"""
        specs = {}
        
        # Look for specification table
        spec_rows = soup.select('.spec-table tr, .specifications tr')
        
        for row in spec_rows:
            cells = row.select('td, th')
            if len(cells) >= 2:
                key = self.extract_text(cells[0])
                value = self.extract_text(cells[1])
                if key and value:
                    specs[key] = value
        
        return specs
    
    def extract_category(self, soup):
        """Extract danh má»¥c sáº£n pháº©m"""
        # Look for breadcrumb
        breadcrumb = soup.select('.breadcrumb a, .category-path a')
        if breadcrumb and len(breadcrumb) > 1:
            return self.extract_text(breadcrumb[-1])
        
        return ""
    
    def filter_products(self, **filters):
        """Lá»c sáº£n pháº©m theo Ä‘iá»u kiá»‡n"""
        filtered = self.products.copy()
        
        if 'min_price' in filters:
            filtered = [p for p in filtered if p.price >= filters['min_price']]
        
        if 'max_price' in filters:
            filtered = [p for p in filtered if p.price <= filters['max_price']]
        
        if 'min_rating' in filters:
            filtered = [p for p in filtered if p.rating >= filters['min_rating']]
        
        if 'min_discount' in filters:
            filtered = [p for p in filtered if p.discount_percent >= filters['min_discount']]
        
        if 'category' in filters:
            filtered = [p for p in filtered if filters['category'].lower() in p.category.lower()]
        
        return filtered
    
    def sort_products(self, by='price', ascending=True):
        """Sáº¯p xáº¿p sáº£n pháº©m"""
        sort_key_map = {
            'price': lambda p: p.price,
            'rating': lambda p: p.rating,
            'discount': lambda p: p.discount_percent,
            'reviews': lambda p: p.review_count,
            'name': lambda p: p.name.lower()
        }
        
        if by in sort_key_map:
            return sorted(self.products, key=sort_key_map[by], reverse=not ascending)
        
        return self.products
    
    def get_price_analysis(self):
        """PhÃ¢n tÃ­ch giÃ¡ cáº£"""
        if not self.products:
            return {}
        
        prices = [p.price for p in self.products if p.price > 0]
        
        if not prices:
            return {}
        
        return {
            'min_price': min(prices),
            'max_price': max(prices),
            'avg_price': sum(prices) / len(prices),
            'median_price': sorted(prices)[len(prices) // 2],
            'total_products': len(self.products),
            'price_ranges': self.get_price_ranges(prices)
        }
    
    def get_price_ranges(self, prices):
        """PhÃ¢n tÃ­ch phÃ¢n bá»‘ giÃ¡"""
        max_price = max(prices)
        ranges = {
            'under_100k': 0,
            '100k_500k': 0,
            '500k_1m': 0,
            '1m_5m': 0,
            'over_5m': 0
        }
        
        for price in prices:
            if price < 100000:
                ranges['under_100k'] += 1
            elif price < 500000:
                ranges['100k_500k'] += 1
            elif price < 1000000:
                ranges['500k_1m'] += 1
            elif price < 5000000:
                ranges['1m_5m'] += 1
            else:
                ranges['over_5m'] += 1
        
        return ranges
    
    def generate_product_report(self):
        """Táº¡o bÃ¡o cÃ¡o sáº£n pháº©m"""
        report = {
            'summary': {
                'total_products': len(self.products),
                'scraping_time': datetime.now().isoformat(),
                'site': self.site_name
            },
            'price_analysis': self.get_price_analysis(),
            'top_rated': sorted([p for p in self.products if p.rating > 0], 
                               key=lambda p: p.rating, reverse=True)[:5],
            'best_deals': sorted([p for p in self.products if p.discount_percent > 0], 
                                key=lambda p: p.discount_percent, reverse=True)[:5],
            'scraping_stats': self.get_statistics()
        }
        
        return report

# Demo E-commerce Scraper
def demo_ecommerce_scraper():
    """Demo E-commerce Scraper"""
    print("ğŸ›’ DEMO E-COMMERCE SCRAPER")
    print("=" * 50)
    
    # Táº¡o scraper
    scraper = EcommerceScraper("https://shopee.vn", "Shopee")
    
    # Táº¡o dá»¯ liá»‡u demo
    demo_products = [
        Product(
            name="iPhone 15 Pro Max 256GB",
            price=29990000,
            original_price=32990000,
            discount_percent=9.1,
            rating=4.8,
            review_count=1250,
            url="https://shopee.vn/iphone-15-pro-max",
            image_url="https://example.com/iphone15.jpg",
            description="iPhone 15 Pro Max má»›i nháº¥t vá»›i chip A17 Pro",
            seller="Apple Store",
            category="Äiá»‡n thoáº¡i",
            in_stock=True,
            specifications={
                "MÃ n hÃ¬nh": "6.7 inch Super Retina XDR",
                "Chip": "A17 Pro",
                "RAM": "8GB",
                "Bá»™ nhá»›": "256GB"
            }
        ),
        Product(
            name="Samsung Galaxy S24 Ultra 512GB",
            price=28990000,
            original_price=31990000,
            discount_percent=9.4,
            rating=4.7,
            review_count=890,
            url="https://shopee.vn/samsung-s24-ultra",
            image_url="https://example.com/s24ultra.jpg",
            description="Samsung Galaxy S24 Ultra vá»›i S Pen tÃ­ch há»£p",
            seller="Samsung Store",
            category="Äiá»‡n thoáº¡i",
            in_stock=True,
            specifications={
                "MÃ n hÃ¬nh": "6.8 inch Dynamic AMOLED",
                "Chip": "Snapdragon 8 Gen 3",
                "RAM": "12GB",
                "Bá»™ nhá»›": "512GB"
            }
        ),
        Product(
            name="MacBook Air M3 13 inch 256GB",
            price=27990000,
            original_price=29990000,
            discount_percent=6.7,
            rating=4.9,
            review_count=456,
            url="https://shopee.vn/macbook-air-m3",
            image_url="https://example.com/macbook-air.jpg",
            description="MacBook Air M3 siÃªu má»ng, hiá»‡u nÄƒng cao",
            seller="Apple Store",
            category="Laptop",
            in_stock=True,
            specifications={
                "MÃ n hÃ¬nh": "13.6 inch Liquid Retina",
                "Chip": "Apple M3",
                "RAM": "8GB",
                "SSD": "256GB"
            }
        )
    ]
    
    scraper.products = demo_products
    
    print(f"âœ… Loaded {len(demo_products)} demo products")
    
    # Demo filtering
    print(f"\nğŸ” Lá»ŒC Sáº¢N PHáº¨M:")
    expensive_products = scraper.filter_products(min_price=25000000)
    print(f"Sáº£n pháº©m trÃªn 25 triá»‡u: {len(expensive_products)}")
    
    high_rated = scraper.filter_products(min_rating=4.8)
    print(f"Sáº£n pháº©m rating >= 4.8: {len(high_rated)}")
    
    # Demo sorting
    print(f"\nğŸ“Š Sáº®P Xáº¾P:")
    by_price = scraper.sort_products(by='price', ascending=False)
    print("Sáº¯p xáº¿p theo giÃ¡ (cao â†’ tháº¥p):")
    for product in by_price:
        print(f"  ğŸ’° {product.name}: {product.price:,.0f}Ä‘")
    
    # Price analysis
    print(f"\nğŸ’¹ PHÃ‚N TÃCH GIÃ:")
    analysis = scraper.get_price_analysis()
    print(f"GiÃ¡ tháº¥p nháº¥t: {analysis['min_price']:,.0f}Ä‘")
    print(f"GiÃ¡ cao nháº¥t: {analysis['max_price']:,.0f}Ä‘")
    print(f"GiÃ¡ trung bÃ¬nh: {analysis['avg_price']:,.0f}Ä‘")
    
    # Generate report
    print(f"\nğŸ“‹ BÃO CÃO:")
    report = scraper.generate_product_report()
    print(f"Tá»•ng sáº£n pháº©m: {report['summary']['total_products']}")
    print(f"Top rated products:")
    for product in report['top_rated']:
        print(f"  â­ {product.name}: {product.rating}/5")

# Cháº¡y demo
demo_ecommerce_scraper()
```

### 4. Scheduler vÃ  Automation

```python
import schedule
import threading
from datetime import datetime, timedelta

class ScrapingScheduler:
    """
    Scheduler Ä‘á»ƒ tá»± Ä‘á»™ng cháº¡y scraping tasks
    Giá»‘ng nhÆ° má»™t trá»£ lÃ½ cÃ¡ nhÃ¢n tá»± Ä‘á»™ng!
    """
    
    def __init__(self):
        self.jobs = []
        self.is_running = False
        self.scheduler_thread = None
        self.logger = logging.getLogger(__name__)
    
    def add_daily_job(self, scraper_func, time_str, *args, **kwargs):
        """ThÃªm job cháº¡y hÃ ng ngÃ y"""
        job = schedule.every().day.at(time_str).do(scraper_func, *args, **kwargs)
        self.jobs.append(job)
        self.logger.info(f"ğŸ“… Added daily job at {time_str}")
        return job
    
    def add_hourly_job(self, scraper_func, *args, **kwargs):
        """ThÃªm job cháº¡y hÃ ng giá»"""
        job = schedule.every().hour.do(scraper_func, *args, **kwargs)
        self.jobs.append(job)
        self.logger.info(f"â° Added hourly job")
        return job
    
    def add_interval_job(self, scraper_func, minutes, *args, **kwargs):
        """ThÃªm job cháº¡y theo interval"""
        job = schedule.every(minutes).minutes.do(scraper_func, *args, **kwargs)
        self.jobs.append(job)
        self.logger.info(f"â²ï¸ Added job every {minutes} minutes")
        return job
    
    def start_scheduler(self):
        """Báº¯t Ä‘áº§u scheduler"""
        if self.is_running:
            self.logger.warning("âš ï¸ Scheduler is already running")
            return
        
        self.is_running = True
        self.scheduler_thread = threading.Thread(target=self._run_scheduler, daemon=True)
        self.scheduler_thread.start()
        self.logger.info("ğŸš€ Scheduler started")
    
    def stop_scheduler(self):
        """Dá»«ng scheduler"""
        self.is_running = False
        if self.scheduler_thread:
            self.scheduler_thread.join()
        self.logger.info("â¹ï¸ Scheduler stopped")
    
    def _run_scheduler(self):
        """Cháº¡y scheduler loop"""
        while self.is_running:
            schedule.run_pending()
            time.sleep(1)
    
    def get_job_status(self):
        """Láº¥y tráº¡ng thÃ¡i cÃ¡c jobs"""
        status = []
        for job in self.jobs:
            status.append({
                'job': str(job.job_func),
                'next_run': job.next_run,
                'interval': str(job.interval)
            })
        return status

class AutoScraper:
    """
    Há»‡ thá»‘ng scraping tá»± Ä‘á»™ng vá»›i scheduler
    """
    
    def __init__(self):
        self.scheduler = ScrapingScheduler()
        self.scrapers = {}
        self.results_dir = "scraping_results"
        
        # Táº¡o thÆ° má»¥c results
        import os
        os.makedirs(self.results_dir, exist_ok=True)
    
    def register_scraper(self, name, scraper):
        """ÄÄƒng kÃ½ scraper"""
        self.scrapers[name] = scraper
        print(f"âœ… Registered scraper: {name}")
    
    def schedule_news_scraping(self, time_str="09:00"):
        """Láº­p lá»‹ch scrape news hÃ ng ngÃ y"""
        def news_job():
            print(f"\nğŸ“° [{datetime.now()}] Starting scheduled news scraping...")
            
            if 'news' in self.scrapers:
                scraper = self.scrapers['news']
                scraper.scrape_vnexpress_homepage()
                
                # Export results
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                filename = f"news_{timestamp}"
                scraper.export_data('json', f"{self.results_dir}/{filename}")
                
                print(f"âœ… News scraping completed: {len(scraper.articles)} articles")
            else:
                print("âŒ No news scraper registered")
        
        self.scheduler.add_daily_job(news_job, time_str)
    
    def schedule_product_monitoring(self, search_query, interval_minutes=60):
        """Láº­p lá»‹ch monitor sáº£n pháº©m"""
        def product_job():
            print(f"\nğŸ›’ [{datetime.now()}] Starting product monitoring for: {search_query}")
            
            if 'ecommerce' in self.scrapers:
                scraper = self.scrapers['ecommerce']
                scraper.scrape_product_list(search_query, max_pages=2)
                
                # Export results
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                filename = f"products_{search_query}_{timestamp}"
                
                # Save as JSON with price history
                products_data = []
                for product in scraper.products:
                    products_data.append({
                        'name': product.name,
                        'price': product.price,
                        'original_price': product.original_price,
                        'discount_percent': product.discount_percent,
                        'rating': product.rating,
                        'url': product.url,
                        'scraped_at': datetime.now().isoformat()
                    })
                
                scraper.save_to_json(products_data, f"{self.results_dir}/{filename}.json")
                
                print(f"âœ… Product monitoring completed: {len(scraper.products)} products")
            else:
                print("âŒ No e-commerce scraper registered")
        
        self.scheduler.add_interval_job(product_job, interval_minutes)
    
    def start_auto_scraping(self):
        """Báº¯t Ä‘áº§u auto scraping"""
        print("ğŸ¤– STARTING AUTO SCRAPING SYSTEM")
        print("=" * 40)
        
        self.scheduler.start_scheduler()
        
        # Show scheduled jobs
        jobs = self.scheduler.get_job_status()
        print(f"ğŸ“‹ Scheduled Jobs ({len(jobs)}):")
        for i, job in enumerate(jobs, 1):
            print(f"  {i}. {job['job']} - Next run: {job['next_run']}")
    
    def stop_auto_scraping(self):
        """Dá»«ng auto scraping"""
        self.scheduler.stop_scheduler()
        print("â¹ï¸ Auto scraping stopped")

# Demo Auto Scraper
def demo_auto_scraper():
    """Demo Auto Scraping System"""
    print("ğŸ¤– DEMO AUTO SCRAPING SYSTEM")
    print("=" * 50)
    
    # Táº¡o auto scraper
    auto_scraper = AutoScraper()
    
    # Táº¡o vÃ  Ä‘Äƒng kÃ½ scrapers
    news_scraper = NewsScraper("https://vnexpress.net")
    ecommerce_scraper = EcommerceScraper("https://shopee.vn", "Shopee")
    
    auto_scraper.register_scraper('news', news_scraper)
    auto_scraper.register_scraper('ecommerce', ecommerce_scraper)
    
    # Láº­p lá»‹ch jobs
    print("\nğŸ“… SCHEDULING JOBS:")
    auto_scraper.schedule_news_scraping("10:00")  # Scrape news at 10:00 AM daily
    auto_scraper.schedule_product_monitoring("smartphone", 30)  # Monitor phones every 30 min
    
    # Start system
    auto_scraper.start_auto_scraping()
    
    print("\nâ° System is running... (Press Ctrl+C to stop)")
    
    try:
        # Simulate running for a short time
        time.sleep(10)
        print("\nğŸ“Š System running normally...")
        time.sleep(5)
    except KeyboardInterrupt:
        pass
    finally:
        auto_scraper.stop_auto_scraping()

# Run demo (comment out to avoid actually running scheduler)
# demo_auto_scraper()
```

## ğŸ¯ TÃ³m táº¯t dá»± Ã¡n

### ğŸ† TÃ­nh nÄƒng Ä‘Ã£ triá»ƒn khai:

1. **ğŸ”§ Base WebScraper Class**
   - Request handling vá»›i retry logic
   - Random delays vÃ  User-Agent rotation
   - Error handling vÃ  logging
   - Statistics tracking

2. **ğŸ“° News Scraper**
   - Thu tháº­p tin tá»©c tá»« trang chá»§
   - Extract ná»™i dung chi tiáº¿t
   - PhÃ¢n loáº¡i vÃ  tag articles
   - Search vÃ  filter functionality

3. **ğŸ›’ E-commerce Scraper**
   - Thu tháº­p thÃ´ng tin sáº£n pháº©m
   - Price monitoring vÃ  analysis
   - Product filtering vÃ  sorting
   - Detailed product information

4. **ğŸ¤– Auto Scraping System**
   - Scheduled scraping jobs
   - Multi-scraper management
   - Automatic data export
   - Monitoring vÃ  reporting

### ğŸ“š Kiáº¿n thá»©c Ä‘Ã£ sá»­ dá»¥ng:

- **Web Technologies**: HTTP requests, HTML parsing, CSS selectors
- **Data Processing**: BeautifulSoup, pandas, JSON/CSV handling
- **OOP Design**: Classes, inheritance, dataclasses
- **Concurrency**: Threading, scheduling
- **Error Handling**: Try/except, retry logic
- **Logging**: Structured logging system
- **File I/O**: Reading/writing various formats

:::tip LÆ°u Ã½ quan trá»ng vá» Web Scraping:
1. **Respect robots.txt** - Kiá»ƒm tra file robots.txt cá»§a website
2. **Rate limiting** - KhÃ´ng spam requests quÃ¡ nhanh
3. **Legal compliance** - TuÃ¢n thá»§ Terms of Service
4. **Ethical scraping** - Chá»‰ scrape dá»¯ liá»‡u cÃ´ng khai cáº§n thiáº¿t
5. **Use APIs when available** - API thÆ°á»ng tá»‘t hÆ¡n scraping
:::

**BÆ°á»›c tiáº¿p theo**: Thá»­ táº¡o cÃ¡c dá»± Ã¡n khÃ¡c Ä‘á»ƒ Ã¡p dá»¥ng nhá»¯ng kiáº¿n thá»©c Ä‘Ã£ há»c!

---
*ğŸ’¡ Tip: Web Scraping lÃ  má»™t ká»¹ nÄƒng máº¡nh máº½, nhÆ°ng hÃ£y sá»­ dá»¥ng cÃ³ trÃ¡ch nhiá»‡m!*

---
**Behitek - Há»c láº­p trÃ¬nh Python má»™t cÃ¡ch dá»… hiá»ƒu nháº¥t! ğŸš€**
